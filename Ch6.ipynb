{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from common import GridWorld\n",
    "from utils import greedy_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.1.2 TD法の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TdAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.01\n",
    "        self.action_size = 4\n",
    "        \n",
    "        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        self.V = defaultdict(lambda: 0)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def eval(self, state, reward, next_state, done):\n",
    "        next_V = 0 if done else self.V[next_state]\n",
    "        target = reward + self.gamma*next_V\n",
    "        \n",
    "        self.V[state] += (target - self.V[state])*self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.TdAgent.__init__.<locals>.<lambda>()>,\n",
       "            {(2, 0): -0.10722226292950292,\n",
       "             (2, 1): -0.24251951645440087,\n",
       "             (2, 2): -0.46968251819524465,\n",
       "             (2, 3): -0.8112040316129755,\n",
       "             (1, 2): -0.5882644418091605,\n",
       "             (1, 3): -0.38930354071851575,\n",
       "             (1, 0): -0.031718344531933285,\n",
       "             (0, 0): 0.03451915424029928,\n",
       "             (0, 1): 0.10577846459535503,\n",
       "             (0, 2): 0.20606555864864864})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = TdAgent()\n",
    "\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.eval(state, reward, next_state, done)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "agent.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.2.2 SARSAの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "        \n",
    "        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = deque(maxlen=2)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def update(self, state, action, reward, done):\n",
    "        self.memory.append((state, action ,reward, done))\n",
    "        if len(self.memory)<2:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _ = self.memory[1]\n",
    "        \n",
    "        next_q = 0 if done else self.Q[next_state, next_action]\n",
    "        \n",
    "        target = reward + self.gamma*next_q\n",
    "        self.Q[state, action] += (target - self.Q[state, action])*self.alpha\n",
    "        \n",
    "        self.pi[state] = greedy_probs(self.Q, state, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.SarsaAgent.__init__.<locals>.<lambda>()>,\n",
       "            {((1, 0), 1): 0.35476355631601536,\n",
       "             ((2, 0), 0): 0.6284124240829533,\n",
       "             ((2, 0), 1): 0.34810778257609504,\n",
       "             ((2, 0), 2): 0.3630650086562598,\n",
       "             ((2, 0), 3): 0.19346065671046364,\n",
       "             ((1, 0), 0): 0.7237957928643892,\n",
       "             ((1, 0), 2): 0.5122046996314671,\n",
       "             ((1, 0), 3): 0.3791278696182847,\n",
       "             ((2, 1), 0): 0.3503393639107582,\n",
       "             ((2, 1), 2): 0.521867198739351,\n",
       "             ((2, 1), 1): 0.2887861848897876,\n",
       "             ((2, 1), 3): 0.2926419869757788,\n",
       "             ((0, 0), 1): 0.5632789945374181,\n",
       "             ((0, 0), 0): 0.4666820560864613,\n",
       "             ((0, 0), 2): 0.4304051059767292,\n",
       "             ((0, 0), 3): 0.8096454023993883,\n",
       "             ((2, 2), 3): 0.16749274117335022,\n",
       "             ((2, 3), 0): -0.8,\n",
       "             ((2, 2), 0): -0.27722048563953816,\n",
       "             ((2, 2), 1): 0.15087458879339855,\n",
       "             ((2, 2), 2): 0.2802740803796917,\n",
       "             ((1, 3), 3): -0.96,\n",
       "             ((2, 3), 1): 0.0,\n",
       "             ((2, 3), 2): 0.16777945058859878,\n",
       "             ((2, 3), 3): 0.0,\n",
       "             ((1, 3), 2): 0.7539609599973439,\n",
       "             ((1, 3), 0): 0.9999872,\n",
       "             ((1, 3), 1): 0.08313693085125158,\n",
       "             ((1, 2), 0): 0.9,\n",
       "             ((0, 2), 3): 1.0,\n",
       "             ((1, 2), 1): 0.18339243648844525,\n",
       "             ((1, 2), 2): 0.5817665488325271,\n",
       "             ((1, 2), 3): -1.3960656374359657,\n",
       "             ((0, 2), 0): 0.8999999998198178,\n",
       "             ((0, 2), 1): 0.81,\n",
       "             ((0, 2), 2): 0.7082880572216915,\n",
       "             ((0, 1), 0): 0.600553534084247,\n",
       "             ((0, 1), 3): 0.8999999999410174,\n",
       "             ((0, 1), 1): 0.7664382262220834,\n",
       "             ((0, 1), 2): 0.4734699657613922})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = SarsaAgent()\n",
    "\n",
    "episodes = 10000\n",
    "for eisode in range(episodes):\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.update(state, action, reward, done)\n",
    "        \n",
    "        if done:\n",
    "            agent.update(state, None, None, None)\n",
    "            break\n",
    "        state = next_state\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.3.2 方策オフ型のSARSAの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaOffPolicyAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        self.b = defaultdict(lambda: random_actions)\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = deque(maxlen = 2)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def update(self, state, action, reward, done):\n",
    "        self.memory.append((state, action, reward, done))\n",
    "        if len(self.memory)<2:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _  = self.memory[1]\n",
    "        \n",
    "        if done:\n",
    "            next_q = 0\n",
    "            rho = 1\n",
    "        else:\n",
    "            next_q = self.Q[next_state, next_action]\n",
    "            rho = self.pi[next_state][next_action]/self.b[next_state][next_action]\n",
    "            \n",
    "        target = rho*(reward + self.gamma*next_q)\n",
    "        self.Q[state, action] += (target - self.Q[state, action])*self.alpha\n",
    "        \n",
    "        self.pi[state] = greedy_probs(self.Q, state, 0)\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.4.3 Q学習の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "        \n",
    "        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        self.b = defaultdict(lambda: random_actions)\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "        \n",
    "        target = reward + self.gamma*next_q_max\n",
    "        self.Q[state, action] += (target - self.Q[state, action])*self.alpha\n",
    "        \n",
    "        self.pi[state] = greedy_probs(self.Q, state, epsilon=0)\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.QLearningAgent.__init__.<locals>.<lambda>()>,\n",
       "            {((2, 0), 0): 0.6561000000000001,\n",
       "             ((2, 0), 1): 0.5904900000000002,\n",
       "             ((2, 0), 2): 0.5904900000000002,\n",
       "             ((2, 0), 3): 0.6561000000000001,\n",
       "             ((2, 1), 0): 0.6561000000000001,\n",
       "             ((2, 1), 1): 0.6561000000000001,\n",
       "             ((2, 1), 2): 0.5904900000000002,\n",
       "             ((2, 1), 3): 0.7290000000000001,\n",
       "             ((1, 0), 0): 0.7290000000000001,\n",
       "             ((1, 0), 1): 0.5904900000000002,\n",
       "             ((1, 0), 2): 0.6561000000000001,\n",
       "             ((1, 0), 3): 0.6561000000000001,\n",
       "             ((0, 0), 0): 0.7290000000000001,\n",
       "             ((0, 0), 1): 0.6561000000000001,\n",
       "             ((0, 0), 2): 0.7290000000000001,\n",
       "             ((0, 0), 3): 0.81,\n",
       "             ((0, 1), 0): 0.81,\n",
       "             ((0, 1), 1): 0.81,\n",
       "             ((0, 1), 2): 0.7290000000000001,\n",
       "             ((0, 1), 3): 0.9,\n",
       "             ((0, 2), 0): 0.9,\n",
       "             ((0, 2), 1): 0.81,\n",
       "             ((0, 2), 2): 0.81,\n",
       "             ((0, 2), 3): 1.0,\n",
       "             ((2, 2), 0): 0.81,\n",
       "             ((2, 2), 1): 0.7290000000000001,\n",
       "             ((2, 2), 2): 0.6561000000000001,\n",
       "             ((2, 2), 3): 0.6561000000000001,\n",
       "             ((1, 2), 0): 0.9,\n",
       "             ((1, 2), 1): 0.7290000000000001,\n",
       "             ((1, 2), 2): 0.81,\n",
       "             ((1, 2), 3): -0.09999999999999998,\n",
       "             ((2, 3), 0): -0.24,\n",
       "             ((2, 3), 1): 0.6550502400000001,\n",
       "             ((2, 3), 2): 0.7290000000000001,\n",
       "             ((2, 3), 3): 0.6560902582271145,\n",
       "             ((1, 3), 0): 1.0,\n",
       "             ((1, 3), 1): 0.6558900479965603,\n",
       "             ((1, 3), 2): 0.80352,\n",
       "             ((1, 3), 3): -0.09984000058982404})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "episodes = 10000\n",
    "for eisode in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        \n",
    "    def get_action(self, action):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def get_action(self, state):\n",
    "        return np.random.choice(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.5.2 サンプルモデル版のQ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self. action_size = 4\n",
    "        \n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        \n",
    "        else:\n",
    "            qs = [self.Q[state, a] for a in range(self.action_size)]\n",
    "            return np.argmax(qs)\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "            \n",
    "        target = self.gamma*next_q_max + reward\n",
    "        self.Q[state, action] += (target - self.Q[state, action])*self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
